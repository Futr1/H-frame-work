<div align="center">

# Cue RAG: Dynamic multi-output cue memory under H framework for Retrieval-augmented Generation

[![lightning](https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![transformers](https://img.shields.io/badge/Transformers-orange)](https://github.com/huggingface/transformers)
 <a href="https://github.com/pytorch/fairseq/blob/main/LICENSE"><img alt="MIT License" src="https://img.shields.io/badge/license-MIT-blue.svg" /></a>
</div>

Nowadays, LLMs still face hallucinations problem. To this problem, Retrieval-augmented generation (RAG) transfers
non-parametric knowledge to LLMs and has been proven to be an effective means to enhance the accuracy of model
answers. Traditional RAG is effective to support LLMs to find the most relevant surface answer from the vector
library, but often does not work well on deeper reasons related to the input. This paper defines the concept of 
**cued memory**, which removes text with high confidence (even if it is highly relevant) after finding the answer related to the
LLMs input. This helps the LLMs to explore the underlying relevant knowledge that match the input. In this paper,
through the exploration of cued memory, we also propose a new **H-framework** RAG: obtain contextual relationships
in the form of a global dependency encoder, use an information filter to delete the direct information that the model
has already obtained and retain the underlying knowledge that has not been learned, and obtain out into cued memory.
Then the cued memory is sent to the sequential dependency encoder, which can achieve dynamic truncation and
dynamic output of multiple high-quality memories. To the end, by creating an infinite memory pool, all the enhanced
text can be fed back to the memory pool, so as to improves the efficiency of memory pool. Our method out performs
on the public dialogue dataset DailyDialog and the translation dataset JRC-Acquis as well as the locally built private
question-answering dataset. In particular, in the dialogue task, our H framework achieved the best B-1/2 result, and
LLMs with cue memory also achieved **SOTA results** in the noise test. The project code and data related to this article
are publicly published at: https://github.com/Futr1/H-frame-work

<div align=center>
<img src=model.svg width=75% height=75% />
</div>

---

## Setup
Our code is mainly based on âš¡ [PyTorch Lightning]() and ğŸ¤— [Transformers](https://github.com/huggingface/transformers). 

## Quick links

* [Environment](#Environment)
* [Retrieval-Augmented Generation Benchmark](#Retrieval-Augmented)
* [Evaluation](#Evaluation)
* [Licence](#Licence)

### Environment
PyTorch version >= 1.10.0
Python version >= 3.8
For training new models, we did  training on 2*A100+2*A6000.

```bash
conda create -n cue memory  python=3.10.0
conda activate cue memory
bash env.sh
```

### cue memory
This repository contains the source code for this paper
æœ¬æ–‡çš„æ ¸å¿ƒä¸ºcue memoryï¼Œç”¨äºæå–ä¸Šä¸‹æ–‡ä¸­æ— ç›´æ¥å…³è”ä½†è‡³å…³é‡è¦çš„ä¿¡æ¯ï¼Œä¸»è¦æ¶‰åŠåˆ°å»é™¤åŸæœ¬å·²ç»æ¨¡å‹æŒæ¡çš„ä¿¡æ¯å’Œä¿ç•™é—´æ¥ç›¸å…³çš„çŸ¥è¯†ï¼Œæœ¬é¡¹ç›®å°†å¯¹å…¶å®ç°æ­¥éª¤è¿›è¡Œè¯¦ç»†è¯´æ˜ï¼ˆä»¥ç¿»è¯‘ä»»åŠ¡ä¸¾ä¾‹ï¼‰
è„šæœ¬ä½¿ç”¨ Fairseq åŠå…¶ä»–å·¥å…·å®Œæˆæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ä»»åŠ¡ä¸­çš„æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚è¦è¿›å…¥ç›®å½•æ˜¯/mnt/7t/fys/fu_dir/pytorch-CycleGAN-and-pix2pix-master/selfmemory/SelfMemory/fairseq-main/fairseq-main/examples/discriminative_reranking_nmtï¼Œä»¥ä¸‹æ˜¯è„šæœ¬çš„è¯¦ç»†åˆ†æ­¥è¯´æ˜

#### 1.å®šä¹‰æ–‡ä»¶è·¯å¾„å’Œå˜é‡
SOURCE_FILEï¼šæºè¯­è¨€æ–‡æœ¬æ–‡ä»¶ï¼ˆsource.txtï¼‰ã€‚
TARGET_FILEï¼šç›®æ ‡è¯­è¨€æ–‡æœ¬æ–‡ä»¶ï¼ˆtest.txtï¼‰ã€‚
HYPO_FILEï¼šå­˜å‚¨æ¨¡å‹é¢„æµ‹çš„å‡è®¾æ–‡ä»¶ï¼ˆoutput.txtï¼‰ã€‚
XLMR_DIRï¼šåŒ…å« SentencePiece æ¨¡å‹å’Œå…¶ä»–èµ„æºçš„ç›®å½•è·¯å¾„ã€‚
OUTPUT_DIRï¼šä¿å­˜å¤„ç†åçš„è¾“å‡ºæ–‡ä»¶å’Œè®­ç»ƒç»“æœçš„ç›®å½•ã€‚
SPLITï¼šæŒ‡å®šè¦å¤„ç†çš„æ•°æ®é›†åˆ†å‰²ï¼ˆtrainã€testã€validï¼‰ã€‚
Nï¼šç”Ÿæˆçš„ beam search å‡è®¾æ•°é‡ï¼Œä¹Ÿå°±æ˜¯ç›®æ ‡è¯­è¨€æ–‡æœ¬å’Œé¢„æµ‹å‡è®¾æ–‡ä»¶å¯¹åº”çš„å€æ•°ï¼Œä¸€å®šè¦ä¸€è‡´ã€‚
NUM_SHARDSï¼šæ•°æ®åˆ†ç‰‡çš„æ•°é‡ï¼Œç”¨äºå¹¶è¡Œå¤„ç†ã€‚
METRICï¼šè¯„ä¼°æŒ‡æ ‡ï¼Œé»˜è®¤è®¾ç½®ä¸º "bleu"ã€‚

#### 2. ä½¿ç”¨ prep_data.py è¿›è¡Œæ•°æ®å‡†å¤‡ï¼Œæ•°æ®ä¼šä¿å­˜åœ¨/mnt/7t/fys/fu_dir/pytorch-CycleGAN-and-pix2pix-master/selfmemory/SelfMemory/fairseq-main/fairseq-main/examples/discriminative_reranking_nmt/out2ä¸­
é€šè¿‡è¿è¡Œ scripts/prep_data.py è„šæœ¬æ¥é¢„å¤„ç†æ¯ä¸ªåˆ†å‰²çš„æ•°æ®ã€‚è¯¥è„šæœ¬æ¥æ”¶æºã€ç›®æ ‡å’Œå‡è®¾æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆè®­ç»ƒå’Œè¯„ä¼°æ‰€éœ€çš„è¾“å…¥ã€‚
python scripts/prep_data.py \
    --input-source ${SOURCE_FILE} \
    --input-target ${TARGET_FILE} \
    --input-hypo ${HYPO_FILE} \
    --output-dir ${OUTPUT_DIR} \
    --split $SPLIT \
    --beam $N \
    --sentencepiece-model ${XLMR_DIR}/sentencepiece.bpe.model \
    --num-shards ${NUM_SHARDS}
è¯¥è„šæœ¬åˆ†åˆ«é’ˆå¯¹æ¯ä¸ªæ•°æ®åˆ†å‰²ï¼ˆtrainã€testã€validï¼‰è¿è¡Œã€‚ä¹Ÿå°±æ˜¯è¿è¡Œäº†ä¸‰æ¬¡ï¼Œout2/blue/splite{N}è·¯å¾„ä¸‹çš„è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†

####3. Fairseq æ•°æ®é¢„å¤„ç†
é’ˆå¯¹ srcï¼ˆæºè¯­è¨€ï¼‰å’Œ tgtï¼ˆç›®æ ‡è¯­è¨€ï¼‰ä½¿ç”¨ fairseq-preprocess å‘½ä»¤é¢„å¤„ç†æ•°æ®ï¼Œä»¥ä¾¿è¿›è¡Œè®­ç»ƒï¼š
for suffix in src tgt ; do
    fairseq-preprocess --only-source \
        --trainpref ${OUTPUT_DIR}/$METRIC/split1/input_${suffix}/train.bpe \
        --validpref ${OUTPUT_DIR}/$METRIC/split1/input_${suffix}/valid.bpe \
        --destdir ${OUTPUT_DIR}/$METRIC/split1/input_${suffix} \
        --workers 60 \
        --srcdict ${XLMR_DIR}/dict.txt
done
æ­¤æ­¥éª¤å°†æ•°æ®è½¬æ¢ä¸º Fairseq çš„äºŒè¿›åˆ¶æ ¼å¼ï¼Œä¾¿äºå¿«é€ŸåŠ è½½ï¼ŒåŒæ—¶ä½¿ç”¨ dict.txt æ–‡ä»¶ä½œä¸ºæºå’Œç›®æ ‡è¯­è¨€çš„å…±äº«å­—å…¸ã€‚

####4. ä¸ºåˆ†ç‰‡æ•°æ®åˆ›å»ºç¬¦å·é“¾æ¥ï¼Œä¹Ÿå°±æ˜¯ä½ çš„åˆ›å»ºæ–‡ä»¶å¤¹é“¾æ¥åˆ°ç¨‹åºçš„æ–‡ä»¶å¤¹ï¼Œå°±ä¸ç”¨ä¿®æ”¹è·¯å¾„äº†
å¯¹äºå…¶ä»–åˆ†ç‰‡ï¼ˆä» split2 å¼€å§‹ï¼‰ï¼Œåˆ›å»ºæŒ‡å‘ split1 éªŒè¯æ–‡ä»¶çš„ç¬¦å·é“¾æ¥ä»¥èŠ‚çœç©ºé—´å’Œæ—¶é—´ï¼š
for i in `seq 2 ${NUM_SHARDS}`; do
    for suffix in src tgt ; do
        fairseq-preprocess --only-source \
            --trainpref ${OUTPUT_DIR}/$METRIC/split${i}/input_${suffix}/train.bpe \
            --destdir ${OUTPUT_DIR}/$METRIC/split${i}/input_${suffix} \
            --workers 60 \
            --srcdict ${XLMR_DIR}/dict.txt

        ln -s ${OUTPUT_DIR}/$METRIC/split1/input_${suffix}/valid* ${OUTPUT_DIR}/$METRIC/split${i}/input_${suffix}/.
    done

    ln -s ${OUTPUT_DIR}/$METRIC/split1/$METRIC/valid* ${OUTPUT_DIR}/$METRIC/split${i}/$METRIC/.
done

####5. ä½¿ç”¨ Fairseq è®­ç»ƒæ¨¡å‹
fairseq-hydra-train å‘½ä»¤ç”¨äºå¯åŠ¨æ¨¡å‹è®­ç»ƒï¼Œå¹¶é…ç½®äº†å¤šä¸ª GPU çš„åˆ†å¸ƒå¼è®­ç»ƒã€‚
fairseq-hydra-train -m \
    --config-dir config/ --config-name deen \
    task.data=${OUTPUT_DIR}/$METRIC/split1/ \
    task.num_data_splits=${NUM_SHARDS} \
    model.pretrained_model=${XLMR_DIR}/model.pt \
    common.user_dir=${FAIRSEQ_ROOT}/examples/discriminative_reranking_nmt \
    checkpoint.save_dir=${EXP_DIR} \
    distributed_training.distributed_world_size=4
æ­¤é…ç½®æŒ‡å‘æ¨¡å‹æ•°æ®ç›®å½•ã€é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä»¥åŠå…¶ä»–åˆ†å¸ƒå¼è®­ç»ƒé€‰é¡¹ã€‚è®­ç»ƒæ•°æ®ä¿å­˜åœ¨/mnt/7t/fys/fu_dir/pytorch-CycleGAN-and-pix2pix-master/selfmemory/SelfMemory/fairseq-main/fairseq-main/examples/discriminative_reranking_nmt/multirun/2024-11-10ä¸­ï¼Œ2024-11-10æ˜¯ä½ çš„è¿è¡Œæ—¶é—´ï¼Œä¼šæ ¹æ®æ—¥æœŸè¿›è¡Œä¿å­˜

The data is putted in `data/`

```text
data/
â”œâ”€â”€ en.json
â”œâ”€â”€ en_refine.json
â”œâ”€â”€ en_int.json
â”œâ”€â”€ en_fact.json
â”œâ”€â”€ zh.json
â”œâ”€â”€ zh_refine.json
â”œâ”€â”€ zh_int.json
â””â”€â”€ zh_fact.json
```

To evalute the Information Integration, you should use `zh_int` or `en_int` for Chinese questions or English questions. 

To evalute the Counterfactual Robustness, you should use `zh_fact` or `en_fact` for Chinese questions or English questions. 

#### The refined data

We refine the retrieved documents and some answers of `en.json` and `zh.json`, and name the new data files as `en_refine.json` and `zh_refine.json`:

+ Removing incorrect positive and negative documents

+ Adding some positive documents.

+ Correcting some inaccurate answers.

### Evaluation

For evaluating ChatGPT, you can run as:

```bash
python evalue.py \
--dataset en \
--modelname chatgpt \
--temp 0.2 \
--noise_rate 0.6 \
--api_key YourAPIKEY \
--passage_num 5
```

For evaluating other models, you can run as:

```bash
python evalue.py \
--dataset en \
--modelname chatglm2-6b \
--temp 0.2 \
--noise_rate 0.6 \
--plm THUDM/chatglm-6b \
--passage_num 5
```

You should change `modelname` and `plm` for different models, where `plm` is the path of model.

`temp` is the temperature of model.

`noise_rate` is rate of noisy documents in inputs.

`passage_num` is number of provided documents for LLM (default is 5).

The outputs are:

+ all_rate: The accuracy (noise_rate<1) or rejection rate (noise_rate=1)
+ fact_check_rate: the error detection rates (ED)

---

To evaluate rejection using ChatGPT, you should first run the `evalue.py` in noise_rate=1 to obtain the generation result, and then run:

```bash
python reject_evalue.py \
--dataset en \
--modelname chatglm2-6b \
--api_key YourAPIKEY
```

The "reject_rate" in the outputs are the reject rate (Rej\*).

---

To evaluate counterfactual robustness using ChatGPT, you should first run the `evalue.py` in dataset=en_fact/zh_fact to obtain the generation result, and then run:

```bash
python fact_evalue.py \
--dataset en_fact \
--modelname chatglm2-6b \
--api_key YourAPIKEY
```

The "reject_rate" in the outputs are the error detection rates (ED\*). The `correct_rate` in the outputs are the error correction rate (CR)

## License

The code and data are released under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License for Noncommercial use only. Any commercial use should get formal permission first.

Shield: [![CC BY-NC-SA 4.0][cc-by-nc-sa-shield]][cc-by-nc-sa]

This work is licensed under a
[Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License][cc-by-nc-sa].

[![CC BY-NC-SA 4.0][cc-by-nc-sa-image]][cc-by-nc-sa]

[cc-by-nc-sa]: http://creativecommons.org/licenses/by-nc-sa/4.0/
[cc-by-nc-sa-image]: https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png
[cc-by-nc-sa-shield]: https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg


