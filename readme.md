<div align="center">

# Cue RAG: Dynamic multi-output cue memory under H framework for Retrieval-augmented Generation

[![lightning](https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![transformers](https://img.shields.io/badge/Transformers-orange)](https://github.com/huggingface/transformers)
 <a href="https://github.com/pytorch/fairseq/blob/main/LICENSE"><img alt="MIT License" src="https://img.shields.io/badge/license-MIT-blue.svg" /></a>
</div>

Nowadays, LLMs still face hallucinations problem. To this problem, Retrieval-augmented generation (RAG) transfers
non-parametric knowledge to LLMs and has been proven to be an effective means to enhance the accuracy of model
answers. Traditional RAG is effective to support LLMs to find the most relevant surface answer from the vector
library, but often does not work well on deeper reasons related to the input. This paper defines the concept of 
**cued memory**, which removes text with high confidence (even if it is highly relevant) after finding the answer related to the
LLMs input. This helps the LLMs to explore the underlying relevant knowledge that match the input. In this paper,
through the exploration of cued memory, we also propose a new **H-framework** RAG: obtain contextual relationships
in the form of a global dependency encoder, use an information filter to delete the direct information that the model
has already obtained and retain the underlying knowledge that has not been learned, and obtain out into cued memory.
Then the cued memory is sent to the sequential dependency encoder, which can achieve dynamic truncation and
dynamic output of multiple high-quality memories. To the end, by creating an infinite memory pool, all the enhanced
text can be fed back to the memory pool, so as to improves the efficiency of memory pool. Our method out performs
on the public dialogue dataset DailyDialog and the translation dataset JRC-Acquis as well as the locally built private
question-answering dataset. In particular, in the dialogue task, our H framework achieved the best B-1/2 result, and
LLMs with cue memory also achieved **SOTA results** in the noise test. The project code and data related to this article
are publicly published at: https://github.com/Futr1/H-frame-work

<div align=center>
<img src=model.svg width=75% height=75% />
</div>

---

## Setup
Our code is mainly based on âš¡ [PyTorch Lightning]() and ğŸ¤— [Transformers](https://github.com/huggingface/transformers). 



cue memory  This repository contains the source code for this paper
è¯¥è„šæœ¬ä½¿ç”¨ Fairseq åŠå…¶ä»–å·¥å…·å®Œæˆæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ä»»åŠ¡ä¸­çš„æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚è¦è¿›å…¥ç›®å½•æ˜¯/mnt/7t/fys/fu_dir/pytorch-CycleGAN-and-pix2pix-master/selfmemory/SelfMemory/fairseq-main/fairseq-main/examples/discriminative_reranking_nmtï¼Œä»¥ä¸‹æ˜¯è„šæœ¬çš„è¯¦ç»†åˆ†æ­¥è¯´æ˜
1.å®šä¹‰æ–‡ä»¶è·¯å¾„å’Œå˜é‡
SOURCE_FILEï¼šæºè¯­è¨€æ–‡æœ¬æ–‡ä»¶ï¼ˆsource.txtï¼‰ã€‚
TARGET_FILEï¼šç›®æ ‡è¯­è¨€æ–‡æœ¬æ–‡ä»¶ï¼ˆtest.txtï¼‰ã€‚
HYPO_FILEï¼šå­˜å‚¨æ¨¡å‹é¢„æµ‹çš„å‡è®¾æ–‡ä»¶ï¼ˆoutput.txtï¼‰ã€‚
XLMR_DIRï¼šåŒ…å« SentencePiece æ¨¡å‹å’Œå…¶ä»–èµ„æºçš„ç›®å½•è·¯å¾„ã€‚
OUTPUT_DIRï¼šä¿å­˜å¤„ç†åçš„è¾“å‡ºæ–‡ä»¶å’Œè®­ç»ƒç»“æœçš„ç›®å½•ã€‚
SPLITï¼šæŒ‡å®šè¦å¤„ç†çš„æ•°æ®é›†åˆ†å‰²ï¼ˆtrainã€testã€validï¼‰ã€‚
Nï¼šç”Ÿæˆçš„ beam search å‡è®¾æ•°é‡ï¼Œä¹Ÿå°±æ˜¯ç›®æ ‡è¯­è¨€æ–‡æœ¬å’Œé¢„æµ‹å‡è®¾æ–‡ä»¶å¯¹åº”çš„å€æ•°ï¼Œä¸€å®šè¦ä¸€è‡´ã€‚
NUM_SHARDSï¼šæ•°æ®åˆ†ç‰‡çš„æ•°é‡ï¼Œç”¨äºå¹¶è¡Œå¤„ç†ã€‚
METRICï¼šè¯„ä¼°æŒ‡æ ‡ï¼Œé»˜è®¤è®¾ç½®ä¸º "bleu"ã€‚

2. ä½¿ç”¨ prep_data.py è¿›è¡Œæ•°æ®å‡†å¤‡ï¼Œæ•°æ®ä¼šä¿å­˜åœ¨/mnt/7t/fys/fu_dir/pytorch-CycleGAN-and-pix2pix-master/selfmemory/SelfMemory/fairseq-main/fairseq-main/examples/discriminative_reranking_nmt/out2ä¸­
é€šè¿‡è¿è¡Œ scripts/prep_data.py è„šæœ¬æ¥é¢„å¤„ç†æ¯ä¸ªåˆ†å‰²çš„æ•°æ®ã€‚è¯¥è„šæœ¬æ¥æ”¶æºã€ç›®æ ‡å’Œå‡è®¾æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆè®­ç»ƒå’Œè¯„ä¼°æ‰€éœ€çš„è¾“å…¥ã€‚
python scripts/prep_data.py \
    --input-source ${SOURCE_FILE} \
    --input-target ${TARGET_FILE} \
    --input-hypo ${HYPO_FILE} \
    --output-dir ${OUTPUT_DIR} \
    --split $SPLIT \
    --beam $N \
    --sentencepiece-model ${XLMR_DIR}/sentencepiece.bpe.model \
    --num-shards ${NUM_SHARDS}
è¯¥è„šæœ¬åˆ†åˆ«é’ˆå¯¹æ¯ä¸ªæ•°æ®åˆ†å‰²ï¼ˆtrainã€testã€validï¼‰è¿è¡Œã€‚ä¹Ÿå°±æ˜¯è¿è¡Œäº†ä¸‰æ¬¡ï¼Œout2/blue/splite{N}è·¯å¾„ä¸‹çš„è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†

3. Fairseq æ•°æ®é¢„å¤„ç†
é’ˆå¯¹ srcï¼ˆæºè¯­è¨€ï¼‰å’Œ tgtï¼ˆç›®æ ‡è¯­è¨€ï¼‰ä½¿ç”¨ fairseq-preprocess å‘½ä»¤é¢„å¤„ç†æ•°æ®ï¼Œä»¥ä¾¿è¿›è¡Œè®­ç»ƒï¼š
for suffix in src tgt ; do
    fairseq-preprocess --only-source \
        --trainpref ${OUTPUT_DIR}/$METRIC/split1/input_${suffix}/train.bpe \
        --validpref ${OUTPUT_DIR}/$METRIC/split1/input_${suffix}/valid.bpe \
        --destdir ${OUTPUT_DIR}/$METRIC/split1/input_${suffix} \
        --workers 60 \
        --srcdict ${XLMR_DIR}/dict.txt
done
æ­¤æ­¥éª¤å°†æ•°æ®è½¬æ¢ä¸º Fairseq çš„äºŒè¿›åˆ¶æ ¼å¼ï¼Œä¾¿äºå¿«é€ŸåŠ è½½ï¼ŒåŒæ—¶ä½¿ç”¨ dict.txt æ–‡ä»¶ä½œä¸ºæºå’Œç›®æ ‡è¯­è¨€çš„å…±äº«å­—å…¸ã€‚

4. ä¸ºåˆ†ç‰‡æ•°æ®åˆ›å»ºç¬¦å·é“¾æ¥ï¼Œä¹Ÿå°±æ˜¯ä½ çš„åˆ›å»ºæ–‡ä»¶å¤¹é“¾æ¥åˆ°ç¨‹åºçš„æ–‡ä»¶å¤¹ï¼Œå°±ä¸ç”¨ä¿®æ”¹è·¯å¾„äº†
å¯¹äºå…¶ä»–åˆ†ç‰‡ï¼ˆä» split2 å¼€å§‹ï¼‰ï¼Œåˆ›å»ºæŒ‡å‘ split1 éªŒè¯æ–‡ä»¶çš„ç¬¦å·é“¾æ¥ä»¥èŠ‚çœç©ºé—´å’Œæ—¶é—´ï¼š
for i in `seq 2 ${NUM_SHARDS}`; do
    for suffix in src tgt ; do
        fairseq-preprocess --only-source \
            --trainpref ${OUTPUT_DIR}/$METRIC/split${i}/input_${suffix}/train.bpe \
            --destdir ${OUTPUT_DIR}/$METRIC/split${i}/input_${suffix} \
            --workers 60 \
            --srcdict ${XLMR_DIR}/dict.txt

        ln -s ${OUTPUT_DIR}/$METRIC/split1/input_${suffix}/valid* ${OUTPUT_DIR}/$METRIC/split${i}/input_${suffix}/.
    done

    ln -s ${OUTPUT_DIR}/$METRIC/split1/$METRIC/valid* ${OUTPUT_DIR}/$METRIC/split${i}/$METRIC/.
done

5. ä½¿ç”¨ Fairseq è®­ç»ƒæ¨¡å‹
fairseq-hydra-train å‘½ä»¤ç”¨äºå¯åŠ¨æ¨¡å‹è®­ç»ƒï¼Œå¹¶é…ç½®äº†å¤šä¸ª GPU çš„åˆ†å¸ƒå¼è®­ç»ƒã€‚
fairseq-hydra-train -m \
    --config-dir config/ --config-name deen \
    task.data=${OUTPUT_DIR}/$METRIC/split1/ \
    task.num_data_splits=${NUM_SHARDS} \
    model.pretrained_model=${XLMR_DIR}/model.pt \
    common.user_dir=${FAIRSEQ_ROOT}/examples/discriminative_reranking_nmt \
    checkpoint.save_dir=${EXP_DIR} \
    distributed_training.distributed_world_size=4
æ­¤é…ç½®æŒ‡å‘æ¨¡å‹æ•°æ®ç›®å½•ã€é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä»¥åŠå…¶ä»–åˆ†å¸ƒå¼è®­ç»ƒé€‰é¡¹ã€‚è®­ç»ƒæ•°æ®ä¿å­˜åœ¨/mnt/7t/fys/fu_dir/pytorch-CycleGAN-and-pix2pix-master/selfmemory/SelfMemory/fairseq-main/fairseq-main/examples/discriminative_reranking_nmt/multirun/2024-11-10ä¸­ï¼Œ2024-11-10æ˜¯ä½ çš„è¿è¡Œæ—¶é—´ï¼Œä¼šæ ¹æ®æ—¥æœŸè¿›è¡Œä¿å­˜

